{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 9\n",
    "## Applied Machine Learning\n",
    "\n",
    "Andrew Chan \n",
    "EBE869"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. [10 pts] Describe the environment in the Nim learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "The environment is a game that provides the following:\n",
    "* 3 piles of cards\n",
    "* For each pile, a maximum of 10 cards per pile\n",
    "\n",
    "Thus, the states are \n",
    "* the number of items in the 1st pile\n",
    "* the number of items in the 2nd pile\n",
    "* the number of items in the 3rd pile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. [10 pts] Describe the agent in the Nim learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "The agent is a player that can perform the following: \n",
    "* select a pile \n",
    "* remove at least 1 card from the pile\n",
    "\n",
    "Thus, the actions are \n",
    "* number of items to remove\n",
    "* pile ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. [10 pts] Describe the reward and penalty in the Nim learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer: \n",
    "The reward is only given upon winning the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. [10 pts] How many possible states there could be in the Nim game with a maximum of 10 items per pile and 3 piles total?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can have 0 cards per pile the total number of states = $${(ITEMS\\_MX + 1)}^{3}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEMS_MX = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1331"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ITEMS_MX+1)**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "Thus, `1331` possible states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. [10 pts] How many possible actions there could be in the Nim game with 10 items per pile and 3 piles total?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we must remove at least 1 card per pile the total number of actions = $${ITEMS\\_MAX}\\times{3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ITEMS_MX)*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "`30` possible actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. [50 pts] Find a way to improve the provided Nim game learning model. \n",
    "Do you think one can beat the Guru player? (Hint: How about penalizing the losses? Hint: It is indeed possible to find a better solution, which improves the way Q-learning updates its Q-table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main updates that I did were the following:\n",
    "* Use epsilon-greedy strategy. \n",
    "* Decay epsilon linearly and using q-table exploitation over number of iterations.\n",
    "* Instead of random, use nim_guru when a uniform random variable is less than epsilon during exploration.\n",
    "* Q learner plays against a nim_guru and receives a -100 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint, choice\n",
    "\n",
    "# The number of piles is 3\n",
    "\n",
    "\n",
    "# max number of items per pile\n",
    "ITEMS_MX = 10\n",
    "\n",
    "# Initialize starting position\n",
    "def init_game():\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]\n",
    "\n",
    "# Based on X-oring the item counts in piles - mathematical solution\n",
    "def nim_guru(st):\n",
    "    xored = st[0] ^ st[1] ^ st[2]\n",
    "    if xored == 0:\n",
    "        return nim_random(st)\n",
    "    #\n",
    "    for pile in range(3):\n",
    "        s = st[pile] ^ xored\n",
    "        if s <= st[pile]:\n",
    "            return st[pile]-s, pile\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st):\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable, Alpha, Gamma, Reward = None, 1.0, 0.8, 100.0\n",
    "\n",
    "epsilon_max = 0.9\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.95\n",
    "epsilon = epsilon_max\n",
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn(_n):\n",
    "    global qtable\n",
    "    global epsilon_max, epsilon_min, epsilon_decay, epsilon\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=float)\n",
    "    # play _n games\n",
    "    for i in range(_n):\n",
    "        # first state is starting position\n",
    "        st1, side = init_game(), 'A'\n",
    "        while True:  # while game not finished\n",
    "            \n",
    "            if side == 'A':\n",
    "            # Q learner plays via epsilon-greedy strategy\n",
    "                ## make a guru move - exploration\n",
    "                if np.random.uniform() < epsilon:\n",
    "                    move, pile = nim_random(st1)\n",
    "                ## make a random move - exploitation\n",
    "                else:\n",
    "                    a = np.argmax(qtable[st1[0], st1[1], st1[2]])  # exploitation\n",
    "                    # index is based on move, pile\n",
    "                    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "                    # check for illegal\n",
    "                    if move <= 0 or st1[pile] < move:\n",
    "                        move, pile = nim_guru(st1)  # exploration\n",
    "\n",
    "                st2 = list(st1)\n",
    "                ## make the move\n",
    "                st2[pile] -= move  # --> last move I made\n",
    "\n",
    "                if st2 == [0, 0, 0]:  # game ends\n",
    "                    qtable_update(Reward, st1, move, pile, 0)  # I won\n",
    "                    break  # new game\n",
    "                qtable_update(0, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "                st1 = st2   \n",
    "            else:\n",
    "            # Guru plays\n",
    "                move, pile = nim_guru(st1)\n",
    "                st2 = list(st1)\n",
    "                st2[pile] -= move\n",
    "                if st2 == [0, 0, 0]:  # game ends\n",
    "                    qtable_update(-100, st1, move, pile, 0)  # I lost\n",
    "                    break  # new game\n",
    "                qtable_update(0, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "                st1 = st2\n",
    "            side = 'B' if side == 'A' else 'A'  # switch sides\n",
    "            \n",
    "            # adjust epsilon\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update(r, _st1, move, pile, q_future_best):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_qlearner(_st):\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "    #\n",
    "    return move, pile  # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Engines = {'Random':nim_random, 'Guru':nim_guru, 'Qlearner':nim_qlearner}\n",
    "\n",
    "def game(a, b):\n",
    "    state, side = init_game(), 'A'\n",
    "    while True:\n",
    "        engine = Engines[a] if side == 'A' else Engines[b]\n",
    "        move, pile = engine(state)\n",
    "        # print(state, move, pile)  # debug purposes\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side\n",
    "        #\n",
    "        side = 'B' if side == 'A' else 'A'  # switch sides\n",
    "\n",
    "def play_games(_n, a, b):\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for i in range(_n):\n",
    "        wins[game(a, b)] += 1\n",
    "    # info\n",
    "    print(f\"{_n} games, {a:>8s}{wins['A']:5d}  {b:>8s}{wins['B']:5d}\")\n",
    "    #\n",
    "    return wins['A'], wins['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nim_qlearn(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner  855    Random  145\n",
      "1000 games,   Random  169  Qlearner  831\n",
      "1000 games,     Guru  937  Qlearner   63\n",
      "1000 games, Qlearner  697      Guru  303\n"
     ]
    }
   ],
   "source": [
    "# Play games\n",
    "play_games(1000, 'Qlearner', 'Random')\n",
    "play_games(1000, 'Random', 'Qlearner') ;\n",
    "play_games(1000, 'Guru', 'Qlearner') ;\n",
    "play_games(1000, 'Qlearner', 'Guru') ;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

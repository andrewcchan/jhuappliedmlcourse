{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 12\n",
    "## Applied Machine Learning\n",
    "\n",
    "Andrew Chan \n",
    "EBE869"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit card fraud costs about 1% to the banks, an amount which customers (us) eventually\n",
    "pay. Let's find those anomalies which might reveal fraud. Download the popular credit card\n",
    "dataset from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. [10 pts] Pre-process the dataset, and then apply normalization or standardization, list number of rows and columns, check sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Locate and load the data file\n",
    "df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "columns_without_target = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9',\n",
    "       'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
    "       'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27',\n",
    "       'V28', 'Amount']\n",
    "df[columns_without_target] = scaler.fit_transform(df[columns_without_target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation for feature reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().sort_values(by='Class', ascending=False)['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting highly correlated features with 'Class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|Feature|Corr|\n",
    "-|-\n",
    "V11    |     0.154876  \n",
    "V4     |     0.133447  \n",
    "V1     |   -0.101347  \n",
    "V18    |  -0.111485  \n",
    "V7     |    -0.187257  \n",
    "V3     |    -0.192961  \n",
    "V16    |    -0.196539  \n",
    "V10    |    -0.216883  \n",
    "V12    |    -0.260593  \n",
    "V14    |    -0.302544  \n",
    "V17    |    -0.326481  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features_with_label = ['V11','V4','V1','V18','V7','V3','V16','V10','V12','V14','V17','Class']\n",
    "selected_features_with_label = ['V10','V12','V14','V17','Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[selected_features_with_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(f'N rows={len(df)}, M columns={len(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. [10 pts] Check the class balance and pick an evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric\n",
    "\n",
    "Since the class balance is heavily skewed towards no fraud, we should use `Recall` (`true positive rate`) since we'd rather error on the side of caution and label fraud versus not. If we miss fraud we could potentially lose a lot of money if a customer cancels their credit card with us:\n",
    "\n",
    "$RECALL = TPR = \\frac{TP}{P} = \\frac{TP}{FN + TP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. [20 pts] Split the dataset 50-50 for training and testing. \n",
    "Then run DecisionTreeClassifier,\n",
    "SVC, MLPClassifier without any tree pruning or regularization. Report the classification\n",
    "performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Class'], axis=1).values\n",
    "y = df['Class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50-50 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf1 = tree.DecisionTreeClassifier(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "fig = plt.subplots(nrows=1, ncols=1,dpi=600)\n",
    "tree.plot_tree(clf1,\n",
    "              class_names=list(map(lambda x: str(x),df['Class'].unique())),\n",
    "              filled = True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf1.predict(X_test)\n",
    "print('RECALL SCORE:',metrics.recall_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf_rbf_svc = svm.SVC(kernel='rbf').fit(X_train, y_train)\n",
    "y_pred = clf_rbf_svc.predict(X_test)\n",
    "print('RECALL SCORE:',metrics.recall_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp1 = MLPClassifier(hidden_layer_sizes=(10,), alpha=0, random_state=None, max_iter=10000).fit(X_train,y_train)\n",
    "y_pred = mlp1.predict(X_test)\n",
    "print('RECALL SCORE:',metrics.recall_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. [20 pts] Run DecisionTreeClassifier, SVC, MLPClassifier with tree pruning and regularization \n",
    "(Hint: Use GridSearchCV to optimize the regularization parameters). Report\n",
    "the classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'ccp_alpha':[1e-12, 1e-9, 1e-6, 1e-3]}\n",
    "clf_dt = tree.DecisionTreeClassifier(random_state=0)\n",
    "clf = GridSearchCV(clf_dt, parameters, scoring = 'recall')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "fig = plt.subplots(nrows=1, ncols=1,dpi=600)\n",
    "tree.plot_tree(clf.best_estimator_,\n",
    "              class_names=list(map(lambda x: str(x),df['Class'].unique())),\n",
    "              filled = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.best_estimator_.predict(X_test)\n",
    "dt_recall_50 = metrics.recall_score(y_test,y_pred)\n",
    "print('RECALL SCORE:', dt_recall_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel':['rbf'], 'C':[1e-3,10,1e3]}\n",
    "clf_rbf_svc = GridSearchCV(svm.SVC(), parameters, scoring = 'recall')\n",
    "clf_rbf_svc.fit(X_train, y_train)\n",
    "y_pred = clf_rbf_svc.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(clf_rbf_svc.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rbf_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(clf_rbf_svc.cv_results_)\n",
    "svc_recall_50 = metrics.recall_score(y_test,y_pred)\n",
    "print('RECALL SCORE:',svc_recall_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'alpha':[1e-6,1e-3,1]}\n",
    "clf_mlp = GridSearchCV(MLPClassifier(hidden_layer_sizes=(10,), random_state=None, max_iter=10000), parameters, scoring = 'recall')\n",
    "clf_mlp.fit(X_train, y_train)\n",
    "y_pred = clf_mlp.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(clf_mlp.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mlp.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(clf_mlp.cv_results_)\n",
    "mlp_recall_50 = metrics.recall_score(y_test,y_pred)\n",
    "print('RECALL SCORE:',mlp_recall_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. [20 pts] Attempt avoiding overfitting while the training is reduced. \n",
    "Add splits 40-60, 30-70, 20-80, 10-90, 5-95 and repeat step 3 and step 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function that will run through all three: DecisionTreeClassifier, SVC, MLPClassifier so we avoid repeated code and bugs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "def trainSweep3Models(test_per, X,y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_per, random_state=0)\n",
    "    \n",
    "    ###################\n",
    "    ## Decision Tree ##\n",
    "    ###################\n",
    "    parameters = {'ccp_alpha':[1e-12, 1e-9, 1e-6, 1e-3]}\n",
    "    clf_dt = GridSearchCV(tree.DecisionTreeClassifier(random_state=0), parameters, scoring = 'recall')\n",
    "    clf_dt.fit(X_train, y_train)\n",
    "    fig = plt.subplots(nrows=1, ncols=1,dpi=600)\n",
    "    tree.plot_tree(clf_dt.best_estimator_,\n",
    "                class_names=list(map(lambda x: str(x), df['Class'].unique())),\n",
    "                filled = True)\n",
    "    plt.show()\n",
    "    y_pred = clf_dt.best_estimator_.predict(X_test)\n",
    "    dt_recall = metrics.recall_score(y_test,y_pred)\n",
    "    print(pd.DataFrame(clf_dt.cv_results_))\n",
    "    print('Decision Tree RECALL SCORE:', dt_recall)\n",
    "    print()\n",
    "\n",
    "    #########\n",
    "    ## SVC ##\n",
    "    #########\n",
    "\n",
    "    parameters = {'kernel':['rbf'], 'C':[1e-3,10,1e3]}\n",
    "    clf_svc = GridSearchCV(svm.SVC(), parameters, scoring = 'recall')\n",
    "    clf_svc.fit(X_train, y_train)\n",
    "    y_pred =    clf_svc.best_estimator_.predict(X_test)\n",
    "    svc_recall = metrics.recall_score(y_test,y_pred)\n",
    "    print(pd.DataFrame(clf_svc.cv_results_))\n",
    "    print('SVC RECALL SCORE:',svc_recall)\n",
    "    print()\n",
    "\n",
    "    #########\n",
    "    ## MLP ##\n",
    "    #########\n",
    "\n",
    "    parameters = {'alpha':[1e-12,1e-9,1e-6,1e-3,1]}\n",
    "    clf_mlp = GridSearchCV(MLPClassifier(hidden_layer_sizes=(10,), random_state=None, max_iter=10000), parameters, scoring = 'recall')\n",
    "    clf_mlp.fit(X_train, y_train)\n",
    "    y_pred = clf_mlp.best_estimator_.predict(X_test)\n",
    "    mlp_recall = metrics.recall_score(y_test,y_pred)\n",
    "    print(pd.DataFrame(clf_mlp.cv_results_))\n",
    "    print('MLP RECALL SCORE:',mlp_recall)\n",
    "    print()\n",
    "\n",
    "    return dt_recall, svc_recall, mlp_recall, clf_dt, clf_svc, clf_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 40-60\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_recall_40, svc_recall_40, mlp_recall_40, clf_dt_40, clf_svc_40, clf_mlp_40 = trainSweep3Models(0.6,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30-70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dt_recall_30, svc_recall_30, mlp_recall_30, clf_dt_30, clf_svc_30, clf_mlp_30 = trainSweep3Models(0.7,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20-80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dt_recall_20, svc_recall_20, mlp_recall_20, clf_dt_20, clf_svc_20, clf_mlp_20 = trainSweep3Models(0.8,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dt_recall_10, svc_recall_10, mlp_recall_10, clf_dt_10, clf_svc_10, clf_mlp_10 = trainSweep3Models(0.9,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dt_recall_5, svc_recall_5, mlp_recall_5, clf_dt_5, clf_svc_5, clf_mlp_5 = trainSweep3Models(0.95,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. [20 pts] Plot everything you have on a single plot and comment about your results in terms of training size, regularization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSize = [0.5,0.4,0.3,0.2,0.1,0.05]\n",
    "recall_dt = [dt_recall_50, dt_recall_40,dt_recall_30,dt_recall_20,dt_recall_10,dt_recall_5]\n",
    "recall_svc = [svc_recall_50, svc_recall_40,svc_recall_30,svc_recall_20,svc_recall_10,svc_recall_5]\n",
    "recall_mlp =  [mlp_recall_50,mlp_recall_40,mlp_recall_30,mlp_recall_20,mlp_recall_10,mlp_recall_5]\n",
    "# Plot\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(trainingSize, recall_dt, ':', color='red', label='Decision Tree')\n",
    "plt.plot(trainingSize, recall_svc, ':', color='green', label='SVC')\n",
    "plt.plot(trainingSize, recall_mlp, ':', color='blue', label='MLP')\n",
    "\n",
    "# Labels\n",
    "plt.title('Recall versus training size')\n",
    "plt.xlabel('Training Percent')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
